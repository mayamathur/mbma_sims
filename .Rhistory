MLo = meta.robu$reg_table$CI.L,
MHi = meta.robu$reg_table$CI.U,
Shat = NA,
SLo = NA,
SHi = NA ) )
},
.rep.res = rep.res )
cat("\n doParallel flag: Done mbma-Mhat-true-t2 if applicable")
}
# using the true muB
if ( "mbma-muB" %in% all.methods ) {
rep.res = run_method_safe(method.label = c("mbma-muB"),
method.fn = function() {
# from inside PublicationBias::corrected_meta;
#  only change is that we want affirm indicator to be that of the *confounded* estimates, not the adjusted ones
# weight for model
weights = rep( 1, length(dp$yi.adj.true) )
# weight based on the affirm indicator of the *confounded* estimates
weights[ dp$affirm == FALSE ] = p$eta
# initialize a dumb (unclustered and uncorrected) version of tau^2
# which is only used for constructing weights
meta.re = rma.uni( yi = dp$yi.adj.true,
vi = dp$vi)
t2hat.naive = meta.re$tau2  # could subtract off the sig2B here, but would also need to account for some studies' being unconfounded
# fit weighted robust model
meta.robu = robu( yi.adj.true ~ 1,
studynum = 1:nrow(dp),
data = dp,
userweights = weights / (vi + t2hat.naive),
var.eff.size = vi,
small = TRUE )
# follow the same return structure as report_meta
list( stats = data.frame( Mhat = as.numeric(meta.robu$b.r),
MLo = meta.robu$reg_table$CI.L,
MHi = meta.robu$reg_table$CI.U,
Shat = NA,
SLo = NA,
SHi = NA ) )
},
.rep.res = rep.res )
cat("\n doParallel flag: Done mbma-muB if applicable")
}
#srr()
# ~~ ********* RTMA WITH CONFOUNDING ADJUSTMENT ------------------------------
# RTMA with true bias parameters
if ( "rtma-adj-muB" %in% all.methods ) {
# # temp for refreshing stan code
# path = "/home/groups/manishad/MBMA"
#setwd(path)
# source("helper_MBMA.R")
# source("init_stan_model_MBMA.R")
# this one has two labels in method arg because a single call to estimate_jeffreys_mcmc
#  returns 2 lines of output, one for posterior mean and one for posterior median
# order of labels in method arg needs to match return structure of estimate_jeffreys_mcmc
rep.res = run_method_safe(method.label = c("rtma-adj-muB-pmean",
"rtma-adj-muB-pmed",
"rtma-adj-muB-max-lp-iterate"),
# note that we're now passing the confounding-adjusted estimates, variances,
#  and critical values
method.fn = function() estimate_jeffreys_mcmc_RTMA(.yi = dpn$yi.adj.true,
.sei = sqrt(dpn$vi.adj.true),
.tcrit = dpn$tcrit.adj.true,
.Mu.start = Mhat.start,
# can't handle start value of 0:
.Tt.start = max(0.01, Shat.start),
.stan.adapt_delta = p$stan.adapt_delta,
.stan.maxtreedepth = p$stan.maxtreedepth), .rep.res = rep.res )
Mhat.MaxLP = rep.res$Mhat[ rep.res$method == "rtma-adj-muB-max-lp-iterate" ]
Shat.MaxLP = rep.res$Shat[ rep.res$method == "rtma-adj-muB-max-lp-iterate" ]
cat("\n doParallel flag: Done rtma-adj-muB if applicable")
}
#srr()
# ~~ Change Starting Values -----
if ( !is.na(Mhat.MaxLP) ) Mhat.start = Mhat.MaxLP
if ( !is.na(Shat.MaxLP) ) Shat.start = Shat.MaxLP
# RTMA with identifiable (estimated) bias parameters
if ( "rtma-adj-MhatB" %in% all.methods ) {
# # temp for refreshing stan code
# path = "/home/groups/manishad/MBMA"
#setwd(path)
# source("helper_MBMA.R")
# source("init_stan_model_MBMA.R")
# this one has two labels in method arg because a single call to estimate_jeffreys_mcmc
#  returns 2 lines of output, one for posterior mean and one for posterior median
# order of labels in method arg needs to match return structure of estimate_jeffreys_mcmc
rep.res = run_method_safe(method.label = c("rtma-adj-MhatB-pmean",
"rtma-adj-MhatB-pmed",
"rtma-adj-MhatB-max-lp-iterate"),
# note that we're now passing the confounding-adjusted estimates, variances,
#  and critical values
method.fn = function() estimate_jeffreys_mcmc_RTMA(.yi = dpn$yi.adj.est,
.sei = sqrt(dpn$vi.adj.est),
.tcrit = dpn$tcrit.adj.est,
.Mu.start = Mhat.start,
# can't handle start value of 0:
.Tt.start = max(0.01, Shat.start),
.stan.adapt_delta = p$stan.adapt_delta,
.stan.maxtreedepth = p$stan.maxtreedepth), .rep.res = rep.res )
Mhat.MaxLP = rep.res$Mhat[ rep.res$method == "rtma-adj-MhatB-max-lp-iterate" ]
Shat.MaxLP = rep.res$Shat[ rep.res$method == "rtma-adj-MhatB-max-lp-iterate" ]
cat("\n doParallel flag: Done rtma-adj-MhatB if applicable")
}
#srr()
# ~~ Change Starting Values -----
if ( !is.na(Mhat.MaxLP) ) Mhat.start = Mhat.MaxLP
if ( !is.na(Shat.MaxLP) ) Shat.start = Shat.MaxLP
# ~~ ****** MAP (SD param) ------------------------------
# THIS IS STILL USING THE TRUE ADJUSTMENT
if ( "jeffreys-adj-sd" %in% all.methods ) {
rep.res = run_method_safe(method.label = c("jeffreys-adj-sd"),
method.fn = function() estimate_jeffreys_RTMA(yi = dpn$yi.adj.true,
sei = sqrt(dpn$vi.adj.true),
par2is = "Tt",
tcrit = dpn$tcrit.adj.true,
Mu.start = Mhat.start,
par2.start = Shat.start,
usePrior = TRUE,
get.CIs = p$get.CIs,
CI.method = "wald",
run.optimx = p$run.optimx
),
.rep.res = rep.res )
Mhat.MAP = rep.res$Mhat[ rep.res$method == "jeffreys-adj-sd" ]
Shat.MAP = rep.res$Shat[ rep.res$method == "jeffreys-adj-sd" ]
}
# ~~ ******** MAON WITH CONFOUNDING ADJUSTMENT --------------------------------------
# using the true muB
if ( "maon-adj-muB" %in% all.methods ) {
rep.res = run_method_safe(method.label = c("maon-adj-muB"),
method.fn = function() {
mod = robu( yi.adj.true ~ 1,
data = dpn,
studynum = 1:nrow(dpn),
var.eff.size = vi,  # using original variance
small = TRUE )
report_meta(mod, .mod.type = "robu")
},
.rep.res = rep.res )
}
# using the sample estimate of muB
if ( "maon-adj-MhatB" %in% all.methods ) {
rep.res = run_method_safe(method.label = c("maon-adj-MhatB"),
method.fn = function() {
mod = robu( yi.adj.est ~ 1,
data = dpn,
studynum = 1:nrow(dpn),
var.eff.size = vi,  # using original variance
small = TRUE )
report_meta(mod, .mod.type = "robu")
},
.rep.res = rep.res )
}
if ( verbose == TRUE ) srr()
# ~ Add Scen Params and Sanity Checks --------------------------------------
# add in scenario parameters
# do NOT use rbind here; bind_cols accommodates possibility that some methods' rep.res
#  have more columns than others
rep.res = p %>% bind_cols( rep.res )
# add more info
rep.res = rep.res %>% add_column( rep.name = i, .before = 1 )
rep.res = rep.res %>% add_column( scen.name = scen, .before = 1 )
rep.res = rep.res %>% add_column( job.name = jobname, .before = 1 )
cat("\ndoParallel flag: Before adding sanity checks to rep.res")
# add info about simulated datasets
rep.res = rep.res %>% add_column(   sancheck.dp.k = nrow(dp),
sancheck.dp.k.affirm = sum(dp$affirm == TRUE),
sancheck.dp.k.nonaffirm = sum(dp$affirm == FALSE),
sancheck.dp.k.conf = sum( dp$Ci == 1 ),
sancheck.dp.k.affirm.conf = ifelse( sum(dp$affirm) > 0, sum( dp$Ci[ dp$affirm == 1 ] ), NA ),
sancheck.dp.k.nonaffirm.conf = ifelse( sum(dp$affirm) < 1, sum( dp$Ci[ dp$affirm == 0 ] ), NA ),
# sanity checks for MhatB
# E[Bi^* | Ci^* = 1], the target for MhatB:
sancheck.MhatB = MhatB,
# note: Fi = 1 here is FAVORING indicator, so this is still the mean Bi among underlying (pre-SAS) estimates
# this is an underlying SAMPLE estimate of the truth; should approximately agree with MhatB and muB
sancheck.EBsti = ifelse( mean(d$Ci == 1 & d$Fi == 1) > 0,
mean( d$Bi[ d$Ci == 1 & d$Fi == 1] ),
NA ),
sancheck.shat2B = shat2B )
# MORE SANITY CHECKS THAT MAY FAIL IF, E.G., CIi=0 ALWAYS:
# # add info about simulated datasets
# # "ustudies"/"udraws" refers to underlying studies/draws prior to hacking or publication bias
# sancheck.prob.published.is.confounded = mean( dp$Ci == 1 )
# sancheck.prob.published.affirm.is.confounded = mean( dp$Ci[ dp$affirm == 1 ] == 1 )
# sancheck.prob.published.nonaffirm.is.confounded = mean( dp$Ci[ dp$affirm == 0 ] == 0 )
#
#
# ( sancheck.prob.ustudies.published =  mean( d.first$study %in% unique(dp$study) ) )
# expect_equal( sancheck.prob.ustudies.published, nrow(dp)/nrow(d.first) )
# # this one should always be 100% unless there's also publication bias:
# ( sancheck.prob.unhacked.ustudies.published =  mean( d.first$study[ d.first$hack == "no" ] %in% unique( dp$study[ dp$hack == "no" ] ) ) )
# # under affirm hacking, will be <100%:
# ( sancheck.prob.hacked.ustudies.published =  mean( d.first$study[ d.first$hack != "no" ] %in% unique( dp$study[ dp$hack != "no" ] ) ) )
#
# # might NOT be 100% if you're generating multiple draws per unhacked studies but favoring, e.g., a random one:
# ( sancheck.prob.unhacked.udraws.published =  mean( d$study.draw[ d$hack == "no" ] %in% unique( dp$study.draw[ dp$hack == "no" ] ) ) )
# ( sancheck.prob.hacked.udraws.published =  mean( d$study.draw[ d$hack != "no" ] %in% unique( dp$study.draw[ dp$hack != "no" ] ) ) )
#
#
#
# #*this one is especially important: under worst-case hacking, it's analogous to prop.retained in
# #  TNE since it's the proportion of the underlying distribution that's nonaffirmative
# ( sancheck.prob.unhacked.udraws.nonaffirm =  mean( d$affirm[ d$hack == "no" ] == FALSE ) )
# # a benchmark for average power:
# ( sancheck.prob.unhacked.udraws.affirm =  mean( d$affirm[ d$hack == "no" ] ) )
# ( sancheck.prob.hacked.udraws.nonaffirm =  mean( d$affirm[ d$hack != "no" ] == FALSE ) )
# ( sancheck.prob.hacked.udraws.affirm =  mean( d$affirm[ d$hack != "no" ] ) )
#
# # probability that a published, nonaffirmative draw is from a hacked study
# # under worst-case hacking, should be 0
# ( sancheck.prob.published.nonaffirm.is.hacked = mean( dp$hack[ dp$affirm == 0 ] != "no" ) )
# # this will be >0
# ( sancheck.prob.published.affirm.is.hacked = mean( dp$hack[ dp$affirm == 1 ] != "no" ) )
#
# rep.res = rep.res %>% add_column(   sancheck.dp.k = nrow(dp),
#                                     sancheck.dp.k.affirm = sum(dp$affirm == TRUE),
#                                     sancheck.dp.k.nonaffirm = sum(dp$affirm == FALSE),
#
#                                     sancheck.prob.published.is.confounded = sancheck.prob.published.is.confounded,
#                                     sancheck.prob.published.affirm.is.confounded = sancheck.prob.published.affirm.is.confounded,
#                                     sancheck.prob.published.nonaffirm.is.confounded = sancheck.prob.published.nonaffirm.is.confounded,
#
#                                     sancheck.dp.k.affirm.unhacked = sum(dp$affirm == TRUE & dp$hack == "no"),
#                                     sancheck.dp.k.affirm.hacked = sum(dp$affirm == TRUE & dp$hack != "no"),
#                                     sancheck.dp.k.nonaffirm.unhacked = sum(dp$affirm == FALSE & dp$hack == "no"),
#                                     sancheck.dp.k.nonaffirm.hacked = sum(dp$affirm == FALSE & dp$hack != "no"),
#
#                                     # means draws per HACKED, published study
#                                     sancheck.dp.meanN.hacked = mean( dp$N[dp$hack != "no"] ),
#                                     sancheck.dp.q90N.hacked = quantile( dp$N[dp$hack != "no"], 0.90 ),
#
#                                     # average yi's of published draws from each study type
#                                     sancheck.mean.yi.unhacked.pub.study = mean( dp$yi[ dp$hack == "no"] ),
#                                     sancheck.mean.yi.hacked.pub.study = mean( dp$yi[ dp$hack != "no"] ),
#
#
#                                     sancheck.mean.mui.unhacked.pub.nonaffirm = mean( dp$mui[ dp$hack == "no" & dp$affirm == FALSE ] ),
#                                     sancheck.mean.yi.unhacked.pub.nonaffirm = mean( dp$yi[ dp$hack == "no" & dp$affirm == FALSE ] ),
#                                     sancheck.mean.yi.unhacked.pub.affirm = mean( dp$yi[ dp$hack == "no" & dp$affirm == TRUE ] ),
#
#                                     sancheck.mean.yi.hacked.pub.nonaffirm = mean( dp$yi[ dp$hack != "no" & dp$affirm == FALSE ] ),
#                                     sancheck.mean.yi.hacked.pub.affirm = mean( dp$yi[ dp$hack != "no" & dp$affirm == TRUE ] ),
#
#                                     # average Zi's
#                                     sancheck.mean.Zi.unhacked.pub.study = mean( dp$Zi[ dp$hack == "no"] ),
#                                     sancheck.mean.Zi.hacked.pub.study = mean( dp$Zi[ dp$hack != "no"] ),
#
#                                     sancheck.mean.Zi.unhacked.pub.nonaffirm = mean( dp$Zi[ dp$hack == "no" & dp$affirm == FALSE ] ),
#                                     sancheck.mean.Zi.unhacked.pub.affirm = mean( dp$Zi[ dp$hack == "no" & dp$affirm == TRUE ] ),
#
#                                     sancheck.mean.Zi.hacked.pub.nonaffirm = mean( dp$Zi[ dp$hack != "no" & dp$affirm == FALSE ] ),
#                                     sancheck.mean.Zi.hacked.pub.affirm = mean( dp$Zi[ dp$hack != "no" & dp$affirm == TRUE ] ),
#
#
#                                     sancheck.prob.ustudies.published = sancheck.prob.ustudies.published,
#                                     sancheck.prob.unhacked.ustudies.published = sancheck.prob.unhacked.ustudies.published,
#                                     sancheck.prob.hacked.ustudies.published = sancheck.prob.hacked.ustudies.published,
#
#                                     sancheck.prob.unhacked.udraws.published = sancheck.prob.unhacked.udraws.published,
#                                     sancheck.prob.hacked.udraws.published = sancheck.prob.hacked.udraws.published,
#
#                                     sancheck.prob.unhacked.udraws.nonaffirm = sancheck.prob.unhacked.udraws.nonaffirm,
#                                     sancheck.prob.unhacked.udraws.affirm = sancheck.prob.unhacked.udraws.affirm,
#                                     sancheck.prob.hacked.udraws.nonaffirm = sancheck.prob.hacked.udraws.nonaffirm,
#                                     sancheck.prob.hacked.udraws.affirm = sancheck.prob.hacked.udraws.affirm,
#
#                                     sancheck.prob.published.nonaffirm.is.hacked = sancheck.prob.published.nonaffirm.is.hacked,
#
#                                     # sanity checks for MhatB
#                                     # E[Bi^* | Ci^* = 1], the target for MhatB:
#                                     sancheck.MhatB = MhatB,
#                                     #@note: Di = 1 here is FAVORING indicator, so this is still the mean Bi among underlying (pre-SAS) estimates
#                                     # this is an underlying SAMPLE estimate of the truth; should approximately agree with MhatB and muB
#                                     sancheck.EBsti = mean( d$Bi[ d$Ci == 1 & d$Di == 1] ),
#
#                                     sancheck.shat2B = shat2B )
rep.res
}  ### end foreach loop for ONE scen in scens.to.run
# aggregate results across multiple scens
#@temp
cat( paste("\n\ndoParallel flag. nrow(new_rs):" ) ); nrow(new_rs)
cat( paste("\n\ndoParallel flag. nrow(rs):" ) ); if (exists("rs")) nrow(rs)
cat( paste("\n\ndoParallel flag. scens.to.run[1]:" ) ); scens.to.run[1]
if ( scen == scens.to.run[1] ) {
cat( paste("\n\ndoParallel flag: IF"))
rs = new_rs
} else {
cat( paste("\n\ndoParallel flag: ELSE"))
rs = bind_rows(rs, new_rs)
}
cat( paste("\n\ndoParallel flag13. nrow(rs):" ) ); if (exists("rs")) nrow(rs)
}
} )[3]  # end system.time
cat( paste("\n\ndoParallel flag. Done with foreach loop." ) )
dim(rs)
scens.to.run = c(1,2,3)
rm(list=ls())
scens.to.run = c(1,2,3)
for ( scen in scens.to.run ) {
new_rs = data.frame( x = c(1,2) )
if ( scen == scens.to.run[1] ) {
rs <<- new_rs
} else {
rs <<- bind_rows(rs, new_rs)
}
} # end "for ( scen in scens.to.run )"
rs
for ( scen in scens.to.run ) {
new_rs = data.frame( x = c(1,2) )
if ( scen == scens.to.run[1] ) {
rs = new_rs
} else {
rs = bind_rows(rs, new_rs)
}
} # end "for ( scen in scens.to.run )"
rs
new_rs = foreach(i = 1:2, .combine=rbind) {
return( data.frame( x = rnorm(n=5) ) )
}
new_rs = foreach(i = 1:2, .combine=rbind) {
data.frame( x = rnorm(n=5) )
}
new_rs = foreach(i = 1:2, .combine=rbind) %dopar% {
data.frame( x = rnorm(n=5) )
}
new_rs
new_rs = foreach(i = 1:2, .combine=rbind) %dopar% {
data.frame( iterate = i, x = rnorm(n=5) )
}
new_rs
new_rs = foreach(i = 1:2, .combine=rbind) %dopar% {
data.frame( scen = scen, iterate = i, x = rnorm(n=5) )
}
new_rs
scens.to.run = c(1,2,3)
for ( scen in scens.to.run ) {
new_rs = foreach(i = 1:2, .combine=rbind) %dopar% {
data.frame( scen = scen, iterate = i, x = rnorm(n=5) )
}
if ( scen == scens.to.run[1] ) {
rs = new_rs
} else {
rs = bind_rows(rs, new_rs)
}
} # end "for ( scen in scens.to.run )"
rs
rm(list=ls())
scens.to.run = c(1,2,3)
for ( scen in scens.to.run ) {
new_rs = foreach(i = 1:2, .combine=rbind) %dopar% {
data.frame( scen = scen, iterate = i, x = rnorm(n=5) )
}
if ( scen == scens.to.run[1] ) {
rs = new_rs
} else {
rs = bind_rows(rs, new_rs)
}
} # end "for ( scen in scens.to.run )"
rs
# set the number of cores
registerDoParallel(cores=16)
scens.to.run = c(1,2,3)
for ( scen in scens.to.run ) {
new_rs = foreach(i = 1:2, .combine=rbind) %dopar% {
data.frame( scen = scen, iterate = i, x = rnorm(n=5) )
}
if ( scen == scens.to.run[1] ) {
rs = new_rs
} else {
rs = bind_rows(rs, new_rs)
}
} # end "for ( scen in scens.to.run )"
rs
# sim.reps is now set only within doParallel
n.scens.per.doParallel = 10
( n.files = n.scen / n.scens.per.doParallel )
# PRELIMINARIES ----------------------------------------------------
#rm(list=ls())
# data-wrangling packages
library(here)
library(plotly)  # must be BEFORE dplyr or else plotly::select will take over
library(dplyr)
library(tibble)
library(ggplot2)
library(data.table)
library(tidyverse)
library(fastDummies)
library(xlsx)
# meta-analysis packages
library(metafor)
library(robumeta)
# other
library(xtable)
library(testthat)
library(Deriv)
library(mosaic)
library(hpa)
library(pracma)
library(truncnorm)
library(tmvtnorm)
library(RColorBrewer)
library(sjmisc)
# prevent masking
select = dplyr::select
# ~~ User-specified global vars -------------------------
# no sci notation
options(scipen=999)
# control which results should be redone and/or overwritten
# e.g., sim_plot_multiple_outcomes
#@ not all fns respect this setting
overwrite.res = TRUE
# ~~ Set directories -------------------------
code.dir = here()
data.dir = str_replace( string = here(),
pattern = "Code",
replacement = "Results/temp" )
# temp while sims are ongoing
results.dir = data.dir
# results.dir = str_replace( string = here(),
#                            pattern = "Code",
#                            replacement = "Results/*2022-7-23 More scens for manuscript; add mbma-MhatB-true-t2" )
#@temp: commented out to avoid overwriting
# overleaf.dir.figs = "/Users/mmathur/Dropbox/Apps/Overleaf/Multiple-bias meta-analysis Overleaf (MBMA)/figures/sims"
#
# # for stats_for_paper.csv
# # same dir as for applied examples so that they'll write to single file
# overleaf.dir.nums = "/Users/mmathur/Dropbox/Apps/Overleaf/Multiple-bias meta-analysis Overleaf (MBMA)/R_objects"
# # alternative for running new simulations
# data.dir = str_replace( string = here(),
#                         pattern = "Code",
#                         replacement = "Results2" )
# results.dir = data.dir
setwd(code.dir)
source("helper_MBMA.R")
source("analyze_sims_helper_MBMA.R")
# ~~ Temp only: check on sims in real time -------------------------
setwd(data.dir)
aggo = fread("agg.csv")
# check when the dataset was last modified to make sure we're working with correct version
file.info("aggo.csv")$mtime
dim(aggo)
nuni(aggo$scen.name)
agg = wrangle_agg_local(aggo)
table(agg$method.pretty)
table(agg$evil.selection)
# which key scen params have run so far?
library(tableone)
param.vars.short = c(
"hack",
"k.pub.nonaffirm",
"prob.hacked",
"eta",
"SAS.type",
"true.dist",
"true.sei.expr",
"muB",
"t2a"
)
CreateCatTable(vars = param.vars.short,
data = agg)
# ~~ List variable names -------------------------
# initialize global variables that describe estimate and outcome names, etc.
# this must be after calling wrangle_agg_local
init_var_names()
# *** BEST AND WORST PERFORMANCE ACROSS SCENS -------------------------
t = agg %>%
filter( method %in% c("naive", "mbma-MhatB", "2psm", "beta-sm") ) %>%
group_by(method) %>%
summarise(BiasMin = min(MhatBias),
BiasMd = median(MhatBias),
BiasMax = max(MhatBias),
AbsBiasMin = min(MhatAbsBias),
AbsBiasMd = median(MhatAbsBias),
AbsBiasMax = max(MhatAbsBias),
MhatEstFail = median(MhatEstFail),
#*express coverage as percent
CoverMin = 100*min(MhatCover),
CoverMd = 100*median(MhatCover),
WidthMd = median(MhatWidth),
WidthMin = min(MhatWidth),
WidthMax = max(MhatWidth))
t
# scens with SWS
temp = agg %>% filter(prob.hacked == 1); dim(temp)
( t1.mn = make_winner_table(.agg = temp,
summarise.fun.name = "mean" ) )
View(t1.mn)
# all scenarios
( t1.mn = make_winner_table(.agg = agg,
summarise.fun.name = "mean" ) )
View(t1.mn)
# scenarios where our method is correctly spec
temp = agg %>% filter(evil.selection == 0); dim(temp)
# scenarios where our method is correctly spec
temp = agg %>% filter(evil.selection == 0); dim(temp)
( t1.mn = make_winner_table(.agg = temp,
summarise.fun.name = "mean" ) )
( t1.worst = make_winner_table(.agg = temp,
summarise.fun.name = "worst10th" ) )
View(t1.mn)
# scenarios where our method is NOT correctly spec
temp = agg %>% filter(evil.selection == 0); dim(temp)
( t1.mn = make_winner_table(.agg = temp,
summarise.fun.name = "mean" ) )
View(t1.mn)
# scenarios where our method is NOT correctly spec
temp = agg %>% filter(evil.selection == 1); dim(temp)
( t1.mn = make_winner_table(.agg = temp,
summarise.fun.name = "mean" ) )
( t1.worst = make_winner_table(.agg = temp,
summarise.fun.name = "worst10th" ) )
View(t1.mn)
