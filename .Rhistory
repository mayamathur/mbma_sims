# index [3] would change for PSM with more cut points
EtaGammaHat = 1/mod$output_adj$par[3]
) )
},
.rep.res = rep.res )
}
rep.res
# then implement the other easy DGP things and make sure they all run locally
# you got this! oh yeah! :)
# ~~ 2PSM (All Published Draws)
if ( "beta-sm" %in% all.methods ) {
rep.res = run_method_safe(method.label = c("beta-sm"),
method.fn = function() {
# must start with naive fit for selmodel
naive = rma( yi = dp$yi,
vi = dp$vi,
method = "REML",
knha = TRUE )
mod = selmodel(x = naive,
type = "beta",
alternative = "two.sided")
# not returning any info about selection parameters because they
#  don't have same interpretation as eta
report_meta(mod, .mod.type = "rma")
# IMPORTANT:
# beta-sm is prone to the warning: "error when trying to invert Hessian"
#  in which case there can be a point estimate with no SEs or tau
#  in this case, selmodel will still return something,
#  but report_meta will throw the missing value error from tau_CI
#  this behavior is what we want, but does mean that the overall.error
#  for beta-sm is not very informative
},
.rep.res = rep.res )
}
rep.res
# ~ New Methods ------------------------------
# ~~ ****** MBMA ------------------------------
# using the identifiable, reweighting-based sample estimate of muB ("MhatB")
if ( "mbma-MhatB" %in% all.methods ) {
rep.res = run_method_safe(method.label = c("mbma-MhatB"),
method.fn = function() {
# from inside PublicationBias::corrected_meta;
#  only change is that we want affirm indicator to be that of the *confounded* estimates, not the adjusted ones
# weight for model
weights = rep( 1, length(dp$yi.adj.est) )
# set weights based on SAS type
# weight based on the affirm indicator of the *confounded* estimates
weights[ dp$affirm == FALSE ] = p$eta  # default
if ( p$SAS.type == "carter_censor" ) {
# this SAS mechanism doesn't use p$eta, so need to get empirical estimate
#**mention in paper
weights[ dp$affirm == FALSE ] = eta_emp
}
# initialize a dumb (unclustered and uncorrected) version of tau^2
# which is only used for constructing weights
meta.re = rma.uni( yi = dp$yi.adj.est,
vi = dp$vi)
t2hat.naive = meta.re$tau2  # could subtract off the sig2B here, but would also need to account for some studies' being unconfounded
# fit weighted robust model
meta.robu = robu( yi.adj.est ~ 1,
studynum = 1:nrow(dp),
data = dp,
userweights = weights / (vi + t2hat.naive),
var.eff.size = vi,
small = TRUE )
# follow the same return structure as report_meta
list( stats = data.frame( Mhat = as.numeric(meta.robu$b.r),
MLo = meta.robu$reg_table$CI.L,
MHi = meta.robu$reg_table$CI.U,
Shat = NA,
SLo = NA,
SHi = NA,
EtaEmpCarter = eta_emp ) )
},
.rep.res = rep.res )
cat("\n doParallel flag: Done mbma-MhatB if applicable")
}
rep.res
# NEW: same as mbma-MhatB, but uses eta*gamma as selection ratio (hard coded for now)
if ( "mbma-MhatB-gamma" %in% all.methods ) {
rep.res = run_method_safe(method.label = c("mbma-MhatB-gamma"),
method.fn = function() {
# from inside PublicationBias::corrected_meta;
#  only change is that we want affirm indicator to be that of the *confounded* estimates, not the adjusted ones
# weight for model
weights = rep( 1, length(dp$yi.adj.est) )
# weight based on the affirm indicator of the *confounded* estimates
# **note this uses the empirical gamma from underlying data to accommodate hacking methods in which
#  we don't specify the population gamma
EtaGammaAssumed = p$eta * dp$gamma[1]
weights[ dp$affirm == FALSE ] = EtaGammaAssumed
# initialize a dumb (unclustered and uncorrected) version of tau^2
# which is only used for constructing weights
meta.re = rma.uni( yi = dp$yi.adj.est,
vi = dp$vi)
t2hat.naive = meta.re$tau2  # could subtract off the sig2B here, but would also need to account for some studies' being unconfounded
# fit weighted robust model
meta.robu = robu( yi.adj.est ~ 1,
studynum = 1:nrow(dp),
data = dp,
userweights = weights / (vi + t2hat.naive),
var.eff.size = vi,
small = TRUE )
# follow the same return structure as report_meta
list( stats = data.frame( Mhat = as.numeric(meta.robu$b.r),
MLo = meta.robu$reg_table$CI.L,
MHi = meta.robu$reg_table$CI.U,
Shat = NA,
SLo = NA,
SHi = NA,
EtaGammaAssumed = EtaGammaAssumed) )
},
.rep.res = rep.res )
cat("\n doParallel flag: Done mbma-MhatB-gamma if applicable")
}
rep.res
# Benchmark: using MhatB but with TRUE tau^2
if ( "mbma-MhatB-true-t2" %in% all.methods ) {
rep.res = run_method_safe(method.label = c("mbma-Mhat-true-t2"),
method.fn = function() {
# from inside PublicationBias::corrected_meta;
#  only change is that we want affirm indicator to be that of the *confounded* estimates, not the adjusted ones
# weight for model
weights = rep( 1, length(dp$yi.adj.est) )
# weight based on the affirm indicator of the *confounded* estimates
weights[ dp$affirm == FALSE ] = p$eta
# fit weighted robust model
meta.robu = robu( yi.adj.est ~ 1,
studynum = 1:nrow(dp),
data = dp,
# here uses true t2:
userweights = weights / (vi + p$t2a + p$t2w),
var.eff.size = vi,
small = TRUE )
# follow the same return structure as report_meta
list( stats = data.frame( Mhat = as.numeric(meta.robu$b.r),
MLo = meta.robu$reg_table$CI.L,
MHi = meta.robu$reg_table$CI.U,
Shat = NA,
SLo = NA,
SHi = NA ) )
},
.rep.res = rep.res )
cat("\n doParallel flag: Done mbma-Mhat-true-t2 if applicable")
}
# using the true muB
if ( "mbma-muB" %in% all.methods ) {
rep.res = run_method_safe(method.label = c("mbma-muB"),
method.fn = function() {
# from inside PublicationBias::corrected_meta;
#  only change is that we want affirm indicator to be that of the *confounded* estimates, not the adjusted ones
# weight for model
weights = rep( 1, length(dp$yi.adj.true) )
# weight based on the affirm indicator of the *confounded* estimates
weights[ dp$affirm == FALSE ] = p$eta
# initialize a dumb (unclustered and uncorrected) version of tau^2
# which is only used for constructing weights
meta.re = rma.uni( yi = dp$yi.adj.true,
vi = dp$vi)
t2hat.naive = meta.re$tau2  # could subtract off the sig2B here, but would also need to account for some studies' being unconfounded
# fit weighted robust model
meta.robu = robu( yi.adj.true ~ 1,
studynum = 1:nrow(dp),
data = dp,
userweights = weights / (vi + t2hat.naive),
var.eff.size = vi,
small = TRUE )
# follow the same return structure as report_meta
list( stats = data.frame( Mhat = as.numeric(meta.robu$b.r),
MLo = meta.robu$reg_table$CI.L,
MHi = meta.robu$reg_table$CI.U,
Shat = NA,
SLo = NA,
SHi = NA ) )
},
.rep.res = rep.res )
cat("\n doParallel flag: Done mbma-muB if applicable")
}
#srr()
# ~~ ********* RTMA WITH CONFOUNDING ADJUSTMENT ------------------------------
# RTMA with true bias parameters
if ( "rtma-adj-muB" %in% all.methods ) {
# # temp for refreshing stan code
# path = "/home/groups/manishad/MBMA"
#setwd(path)
# source("helper_MBMA.R")
# source("init_stan_model_MBMA.R")
# this one has two labels in method arg because a single call to estimate_jeffreys_mcmc
#  returns 2 lines of output, one for posterior mean and one for posterior median
# order of labels in method arg needs to match return structure of estimate_jeffreys_mcmc
rep.res = run_method_safe(method.label = c("rtma-adj-muB-pmean",
"rtma-adj-muB-pmed",
"rtma-adj-muB-max-lp-iterate"),
# note that we're now passing the confounding-adjusted estimates, variances,
#  and critical values
method.fn = function() estimate_jeffreys_mcmc_RTMA(.yi = dpn$yi.adj.true,
.sei = sqrt(dpn$vi.adj.true),
.tcrit = dpn$tcrit.adj.true,
.Mu.start = Mhat.start,
# can't handle start value of 0:
.Tt.start = max(0.01, Shat.start),
.stan.adapt_delta = p$stan.adapt_delta,
.stan.maxtreedepth = p$stan.maxtreedepth), .rep.res = rep.res )
Mhat.MaxLP = rep.res$Mhat[ rep.res$method == "rtma-adj-muB-max-lp-iterate" ]
Shat.MaxLP = rep.res$Shat[ rep.res$method == "rtma-adj-muB-max-lp-iterate" ]
cat("\n doParallel flag: Done rtma-adj-muB if applicable")
}
#srr()
# ~~ Change Starting Values -----
if ( !is.na(Mhat.MaxLP) ) Mhat.start = Mhat.MaxLP
if ( !is.na(Shat.MaxLP) ) Shat.start = Shat.MaxLP
# RTMA with identifiable (estimated) bias parameters
if ( "rtma-adj-MhatB" %in% all.methods ) {
# # temp for refreshing stan code
# path = "/home/groups/manishad/MBMA"
#setwd(path)
# source("helper_MBMA.R")
# source("init_stan_model_MBMA.R")
# this one has two labels in method arg because a single call to estimate_jeffreys_mcmc
#  returns 2 lines of output, one for posterior mean and one for posterior median
# order of labels in method arg needs to match return structure of estimate_jeffreys_mcmc
rep.res = run_method_safe(method.label = c("rtma-adj-MhatB-pmean",
"rtma-adj-MhatB-pmed",
"rtma-adj-MhatB-max-lp-iterate"),
# note that we're now passing the confounding-adjusted estimates, variances,
#  and critical values
method.fn = function() estimate_jeffreys_mcmc_RTMA(.yi = dpn$yi.adj.est,
.sei = sqrt(dpn$vi.adj.est),
.tcrit = dpn$tcrit.adj.est,
.Mu.start = Mhat.start,
# can't handle start value of 0:
.Tt.start = max(0.01, Shat.start),
.stan.adapt_delta = p$stan.adapt_delta,
.stan.maxtreedepth = p$stan.maxtreedepth), .rep.res = rep.res )
Mhat.MaxLP = rep.res$Mhat[ rep.res$method == "rtma-adj-MhatB-max-lp-iterate" ]
Shat.MaxLP = rep.res$Shat[ rep.res$method == "rtma-adj-MhatB-max-lp-iterate" ]
cat("\n doParallel flag: Done rtma-adj-MhatB if applicable")
}
#srr()
# ~~ Change Starting Values -----
if ( !is.na(Mhat.MaxLP) ) Mhat.start = Mhat.MaxLP
if ( !is.na(Shat.MaxLP) ) Shat.start = Shat.MaxLP
# ~~ ****** MAP (SD param) ------------------------------
# THIS IS STILL USING THE TRUE ADJUSTMENT
if ( "jeffreys-adj-sd" %in% all.methods ) {
rep.res = run_method_safe(method.label = c("jeffreys-adj-sd"),
method.fn = function() estimate_jeffreys_RTMA(yi = dpn$yi.adj.true,
sei = sqrt(dpn$vi.adj.true),
par2is = "Tt",
tcrit = dpn$tcrit.adj.true,
Mu.start = Mhat.start,
par2.start = Shat.start,
usePrior = TRUE,
get.CIs = p$get.CIs,
CI.method = "wald",
run.optimx = p$run.optimx
),
.rep.res = rep.res )
Mhat.MAP = rep.res$Mhat[ rep.res$method == "jeffreys-adj-sd" ]
Shat.MAP = rep.res$Shat[ rep.res$method == "jeffreys-adj-sd" ]
}
# ~~ ******** MAON WITH CONFOUNDING ADJUSTMENT --------------------------------------
# using the true muB
if ( "maon-adj-muB" %in% all.methods ) {
rep.res = run_method_safe(method.label = c("maon-adj-muB"),
method.fn = function() {
mod = robu( yi.adj.true ~ 1,
data = dpn,
studynum = 1:nrow(dpn),
var.eff.size = vi,  # using original variance
small = TRUE )
report_meta(mod, .mod.type = "robu")
},
.rep.res = rep.res )
}
# using the sample estimate of muB
if ( "maon-adj-MhatB" %in% all.methods ) {
rep.res = run_method_safe(method.label = c("maon-adj-MhatB"),
method.fn = function() {
mod = robu( yi.adj.est ~ 1,
data = dpn,
studynum = 1:nrow(dpn),
var.eff.size = vi,  # using original variance
small = TRUE )
report_meta(mod, .mod.type = "robu")
},
.rep.res = rep.res )
}
if ( verbose == TRUE ) srr()
# ~ Add Scen Params and Sanity Checks --------------------------------------
# add in scenario parameters
# do NOT use rbind here; bind_cols accommodates possibility that some methods' rep.res
#  have more columns than others
rep.res = p %>% bind_cols( rep.res )
# add more info
rep.res = rep.res %>% add_column( rep.name = i, .before = 1 )
rep.res = rep.res %>% add_column( scen.name = scen, .before = 1 )
rep.res = rep.res %>% add_column( job.name = jobname, .before = 1 )
cat("\ndoParallel flag: Before adding sanity checks to rep.res")
# add info about simulated datasets
rep.res = rep.res %>% add_column(   sancheck.dp.k = nrow(dp),
sancheck.dp.k.affirm = sum(dp$affirm == TRUE),
sancheck.dp.k.nonaffirm = sum(dp$affirm == FALSE),
sancheck.dp.k.conf = sum( dp$Ci == 1 ),
sancheck.dp.k.affirm.conf = ifelse( sum(dp$affirm) > 0, sum( dp$Ci[ dp$affirm == 1 ] ), NA ),
sancheck.dp.k.nonaffirm.conf = ifelse( sum(dp$affirm) < 1, sum( dp$Ci[ dp$affirm == 0 ] ), NA ),
# sanity checks for MhatB
# E[Bi^* | Ci^* = 1], the target for MhatB:
sancheck.MhatB = MhatB,
# note: Fi = 1 here is FAVORING indicator, so this is still the mean Bi among underlying (pre-SAS) estimates
# this is an underlying SAMPLE estimate of the truth; should approximately agree with MhatB and muB
sancheck.EBsti = ifelse( mean(d$Ci == 1 & d$Fi == 1) > 0,
mean( d$Bi[ d$Ci == 1 & d$Fi == 1] ),
NA ),
sancheck.shat2B = shat2B )
# MORE SANITY CHECKS THAT MAY FAIL IF, E.G., CIi=0 ALWAYS:
# # add info about simulated datasets
# # "ustudies"/"udraws" refers to underlying studies/draws prior to hacking or publication bias
# sancheck.prob.published.is.confounded = mean( dp$Ci == 1 )
# sancheck.prob.published.affirm.is.confounded = mean( dp$Ci[ dp$affirm == 1 ] == 1 )
# sancheck.prob.published.nonaffirm.is.confounded = mean( dp$Ci[ dp$affirm == 0 ] == 0 )
#
#
# ( sancheck.prob.ustudies.published =  mean( d.first$study %in% unique(dp$study) ) )
# expect_equal( sancheck.prob.ustudies.published, nrow(dp)/nrow(d.first) )
# # this one should always be 100% unless there's also publication bias:
# ( sancheck.prob.unhacked.ustudies.published =  mean( d.first$study[ d.first$hack == "no" ] %in% unique( dp$study[ dp$hack == "no" ] ) ) )
# # under affirm hacking, will be <100%:
# ( sancheck.prob.hacked.ustudies.published =  mean( d.first$study[ d.first$hack != "no" ] %in% unique( dp$study[ dp$hack != "no" ] ) ) )
#
# # might NOT be 100% if you're generating multiple draws per unhacked studies but favoring, e.g., a random one:
# ( sancheck.prob.unhacked.udraws.published =  mean( d$study.draw[ d$hack == "no" ] %in% unique( dp$study.draw[ dp$hack == "no" ] ) ) )
# ( sancheck.prob.hacked.udraws.published =  mean( d$study.draw[ d$hack != "no" ] %in% unique( dp$study.draw[ dp$hack != "no" ] ) ) )
#
#
#
# #*this one is especially important: under worst-case hacking, it's analogous to prop.retained in
# #  TNE since it's the proportion of the underlying distribution that's nonaffirmative
# ( sancheck.prob.unhacked.udraws.nonaffirm =  mean( d$affirm[ d$hack == "no" ] == FALSE ) )
# # a benchmark for average power:
# ( sancheck.prob.unhacked.udraws.affirm =  mean( d$affirm[ d$hack == "no" ] ) )
# ( sancheck.prob.hacked.udraws.nonaffirm =  mean( d$affirm[ d$hack != "no" ] == FALSE ) )
# ( sancheck.prob.hacked.udraws.affirm =  mean( d$affirm[ d$hack != "no" ] ) )
#
# # probability that a published, nonaffirmative draw is from a hacked study
# # under worst-case hacking, should be 0
# ( sancheck.prob.published.nonaffirm.is.hacked = mean( dp$hack[ dp$affirm == 0 ] != "no" ) )
# # this will be >0
# ( sancheck.prob.published.affirm.is.hacked = mean( dp$hack[ dp$affirm == 1 ] != "no" ) )
#
# rep.res = rep.res %>% add_column(   sancheck.dp.k = nrow(dp),
#                                     sancheck.dp.k.affirm = sum(dp$affirm == TRUE),
#                                     sancheck.dp.k.nonaffirm = sum(dp$affirm == FALSE),
#
#                                     sancheck.prob.published.is.confounded = sancheck.prob.published.is.confounded,
#                                     sancheck.prob.published.affirm.is.confounded = sancheck.prob.published.affirm.is.confounded,
#                                     sancheck.prob.published.nonaffirm.is.confounded = sancheck.prob.published.nonaffirm.is.confounded,
#
#                                     sancheck.dp.k.affirm.unhacked = sum(dp$affirm == TRUE & dp$hack == "no"),
#                                     sancheck.dp.k.affirm.hacked = sum(dp$affirm == TRUE & dp$hack != "no"),
#                                     sancheck.dp.k.nonaffirm.unhacked = sum(dp$affirm == FALSE & dp$hack == "no"),
#                                     sancheck.dp.k.nonaffirm.hacked = sum(dp$affirm == FALSE & dp$hack != "no"),
#
#                                     # means draws per HACKED, published study
#                                     sancheck.dp.meanN.hacked = mean( dp$N[dp$hack != "no"] ),
#                                     sancheck.dp.q90N.hacked = quantile( dp$N[dp$hack != "no"], 0.90 ),
#
#                                     # average yi's of published draws from each study type
#                                     sancheck.mean.yi.unhacked.pub.study = mean( dp$yi[ dp$hack == "no"] ),
#                                     sancheck.mean.yi.hacked.pub.study = mean( dp$yi[ dp$hack != "no"] ),
#
#
#                                     sancheck.mean.mui.unhacked.pub.nonaffirm = mean( dp$mui[ dp$hack == "no" & dp$affirm == FALSE ] ),
#                                     sancheck.mean.yi.unhacked.pub.nonaffirm = mean( dp$yi[ dp$hack == "no" & dp$affirm == FALSE ] ),
#                                     sancheck.mean.yi.unhacked.pub.affirm = mean( dp$yi[ dp$hack == "no" & dp$affirm == TRUE ] ),
#
#                                     sancheck.mean.yi.hacked.pub.nonaffirm = mean( dp$yi[ dp$hack != "no" & dp$affirm == FALSE ] ),
#                                     sancheck.mean.yi.hacked.pub.affirm = mean( dp$yi[ dp$hack != "no" & dp$affirm == TRUE ] ),
#
#                                     # average Zi's
#                                     sancheck.mean.Zi.unhacked.pub.study = mean( dp$Zi[ dp$hack == "no"] ),
#                                     sancheck.mean.Zi.hacked.pub.study = mean( dp$Zi[ dp$hack != "no"] ),
#
#                                     sancheck.mean.Zi.unhacked.pub.nonaffirm = mean( dp$Zi[ dp$hack == "no" & dp$affirm == FALSE ] ),
#                                     sancheck.mean.Zi.unhacked.pub.affirm = mean( dp$Zi[ dp$hack == "no" & dp$affirm == TRUE ] ),
#
#                                     sancheck.mean.Zi.hacked.pub.nonaffirm = mean( dp$Zi[ dp$hack != "no" & dp$affirm == FALSE ] ),
#                                     sancheck.mean.Zi.hacked.pub.affirm = mean( dp$Zi[ dp$hack != "no" & dp$affirm == TRUE ] ),
#
#
#                                     sancheck.prob.ustudies.published = sancheck.prob.ustudies.published,
#                                     sancheck.prob.unhacked.ustudies.published = sancheck.prob.unhacked.ustudies.published,
#                                     sancheck.prob.hacked.ustudies.published = sancheck.prob.hacked.ustudies.published,
#
#                                     sancheck.prob.unhacked.udraws.published = sancheck.prob.unhacked.udraws.published,
#                                     sancheck.prob.hacked.udraws.published = sancheck.prob.hacked.udraws.published,
#
#                                     sancheck.prob.unhacked.udraws.nonaffirm = sancheck.prob.unhacked.udraws.nonaffirm,
#                                     sancheck.prob.unhacked.udraws.affirm = sancheck.prob.unhacked.udraws.affirm,
#                                     sancheck.prob.hacked.udraws.nonaffirm = sancheck.prob.hacked.udraws.nonaffirm,
#                                     sancheck.prob.hacked.udraws.affirm = sancheck.prob.hacked.udraws.affirm,
#
#                                     sancheck.prob.published.nonaffirm.is.hacked = sancheck.prob.published.nonaffirm.is.hacked,
#
#                                     # sanity checks for MhatB
#                                     # E[Bi^* | Ci^* = 1], the target for MhatB:
#                                     sancheck.MhatB = MhatB,
#                                     #@note: Di = 1 here is FAVORING indicator, so this is still the mean Bi among underlying (pre-SAS) estimates
#                                     # this is an underlying SAMPLE estimate of the truth; should approximately agree with MhatB and muB
#                                     sancheck.EBsti = mean( d$Bi[ d$Ci == 1 & d$Di == 1] ),
#
#                                     sancheck.shat2B = shat2B )
rep.res
}  ### end foreach loop for ONE scen in scens.to.run
# aggregate results across multiple scens
#@temp
cat( paste("\n\ndoParallel flag. nrow(new_rs):" ) ); nrow(new_rs)
cat( paste("\n\ndoParallel flag. nrow(rs):" ) ); if (exists("rs")) nrow(rs)
cat( paste("\n\ndoParallel flag. scens.to.run[1]:" ) ); scens.to.run[1]
if ( scen == scens.to.run[1] ) {
cat( paste("\n\ndoParallel flag: IF"))
rs = new_rs
} else {
cat( paste("\n\ndoParallel flag: ELSE"))
rs = bind_rows(rs, new_rs)
}
cat( paste("\n\ndoParallel flag13. nrow(rs):" ) ); if (exists("rs")) nrow(rs)
}
} )[3]  # end system.time
cat( paste("\n\ndoParallel flag. Done with foreach loop." ) )
dim(rs)
scens.to.run = c(1,2,3)
rm(list=ls())
scens.to.run = c(1,2,3)
for ( scen in scens.to.run ) {
new_rs = data.frame( x = c(1,2) )
if ( scen == scens.to.run[1] ) {
rs <<- new_rs
} else {
rs <<- bind_rows(rs, new_rs)
}
} # end "for ( scen in scens.to.run )"
rs
for ( scen in scens.to.run ) {
new_rs = data.frame( x = c(1,2) )
if ( scen == scens.to.run[1] ) {
rs = new_rs
} else {
rs = bind_rows(rs, new_rs)
}
} # end "for ( scen in scens.to.run )"
rs
new_rs = foreach(i = 1:2, .combine=rbind) {
return( data.frame( x = rnorm(n=5) ) )
}
new_rs = foreach(i = 1:2, .combine=rbind) {
data.frame( x = rnorm(n=5) )
}
new_rs = foreach(i = 1:2, .combine=rbind) %dopar% {
data.frame( x = rnorm(n=5) )
}
new_rs
new_rs = foreach(i = 1:2, .combine=rbind) %dopar% {
data.frame( iterate = i, x = rnorm(n=5) )
}
new_rs
new_rs = foreach(i = 1:2, .combine=rbind) %dopar% {
data.frame( scen = scen, iterate = i, x = rnorm(n=5) )
}
new_rs
scens.to.run = c(1,2,3)
for ( scen in scens.to.run ) {
new_rs = foreach(i = 1:2, .combine=rbind) %dopar% {
data.frame( scen = scen, iterate = i, x = rnorm(n=5) )
}
if ( scen == scens.to.run[1] ) {
rs = new_rs
} else {
rs = bind_rows(rs, new_rs)
}
} # end "for ( scen in scens.to.run )"
rs
rm(list=ls())
scens.to.run = c(1,2,3)
for ( scen in scens.to.run ) {
new_rs = foreach(i = 1:2, .combine=rbind) %dopar% {
data.frame( scen = scen, iterate = i, x = rnorm(n=5) )
}
if ( scen == scens.to.run[1] ) {
rs = new_rs
} else {
rs = bind_rows(rs, new_rs)
}
} # end "for ( scen in scens.to.run )"
rs
# set the number of cores
registerDoParallel(cores=16)
scens.to.run = c(1,2,3)
for ( scen in scens.to.run ) {
new_rs = foreach(i = 1:2, .combine=rbind) %dopar% {
data.frame( scen = scen, iterate = i, x = rnorm(n=5) )
}
if ( scen == scens.to.run[1] ) {
rs = new_rs
} else {
rs = bind_rows(rs, new_rs)
}
} # end "for ( scen in scens.to.run )"
rs
# sim.reps is now set only within doParallel
n.scens.per.doParallel = 10
( n.files = n.scen / n.scens.per.doParallel )
