# which is only used for constructing weights
meta.re = rma.uni( yi = dp$yi.adj.true,
vi = dp$vi)
t2hat.naive = meta.re$tau2  #@ could subtract off the sig2B here, but would also need to account for some studies' being unconfounded
# fit weighted robust model
meta.robu = robu( yi.adj.true ~ 1,
studynum = 1:nrow(dp),
data = dp,
userweights = weights / (vi + t2hat.naive),
var.eff.size = vi,
small = TRUE )
# follow the same return structure as report_meta
list( stats = data.frame( Mhat = as.numeric(meta.robu$b.r),
MLo = meta.robu$reg_table$CI.L,
MHi = meta.robu$reg_table$CI.U,
Shat = NA,
SLo = NA,
SHi = NA ) )
},
.rep.res = rep.res )
cat("\n doParallel flag: Done sapb-adj-muB if applicable")
}
# ~~ Benchmark: SAPB WITH TRUE TAU^2
if ( "sapb-true-t2" %in% all.methods ) {
rep.res = run_method_safe(method.label = c("sapb-true-t2"),
method.fn = function() {
# from inside PublicationBias::corrected_meta;
#  only change is that we want affirm indicator to be that of the *confounded* estimates, not the adjusted ones
# weight for model
weights = rep( 1, length(dp$yi) )
# weight based on the affirm indicator of the *confounded* estimates
weights[ dp$affirm == FALSE ] = p$eta
# use the true, unobserved tau^2
t2hat.naive = p$S^2
# fit weighted robust model
meta.robu = robu( yi.adj.true ~ 1,
studynum = 1:nrow(dp),
data = dp,
userweights = weights / (vi + t2hat.naive),
var.eff.size = vi,
small = TRUE )
# follow the same return structure as report_meta
list( stats = data.frame( Mhat = as.numeric(meta.robu$b.r),
MLo = meta.robu$reg_table$CI.L,
MHi = meta.robu$reg_table$CI.U,
Shat = NA,
SLo = NA,
SHi = NA ) )
},
.rep.res = rep.res )
cat("\n doParallel flag: Done sapb-true-t2 if applicable")
}
#srr()
# ~~ ********* RTMA WITH CONFOUNDING ADJUSTMENT ------------------------------
#@ THIS IS STILL USING THE TRUE ADJUSTMENT
if ( "rtma-adj" %in% all.methods ) {
#if ( FALSE ) {
# # temp for refreshing code
# path = "/home/groups/manishad/MBMA"
#setwd(path)
# source("helper_MBMA.R")
# source("init_stan_model_MBMA.R")
# this one has two labels in method arg because a single call to estimate_jeffreys_mcmc
#  returns 2 lines of output, one for posterior mean and one for posterior median
# order of labels in method arg needs to match return structure of estimate_jeffreys_mcmc
rep.res = run_method_safe(method.label = c("rtma-adj-pmean",
"rtma-adj-pmed",
"rtma-adj-max-lp-iterate"),
# note that we're now passing the confounding-adjusted estimates, variances,
#  and critical values
method.fn = function() estimate_jeffreys_mcmc_RTMA(.yi = dpn$yi.adj.true,
.sei = sqrt(dpn$vi.adj.true),
.tcrit = dpn$tcrit.adj.true,
.Mu.start = Mhat.start,
# can't handle start value of 0:
.Tt.start = max(0.01, Shat.start),
.stan.adapt_delta = p$stan.adapt_delta,
.stan.maxtreedepth = p$stan.maxtreedepth), .rep.res = rep.res )
Mhat.MaxLP = rep.res$Mhat[ rep.res$method == "rtma-adj-max-lp-iterate" ]
Shat.MaxLP = rep.res$Shat[ rep.res$method == "rtma-adj-max-lp-iterate" ]
cat("\n doParallel flag: Done rtma-adj if applicable")
}
#srr()
# ~~ Change Starting Values -----
if ( !is.na(Mhat.MaxLP) ) Mhat.start = Mhat.MaxLP
if ( !is.na(Shat.MaxLP) ) Shat.start = Shat.MaxLP
# ~~ ****** MAP (SD param) ------------------------------
if ( "jeffreys-adj-sd" %in% all.methods ) {
rep.res = run_method_safe(method.label = c("jeffreys-adj-sd"),
method.fn = function() estimate_jeffreys_RTMA(yi = dpn$yi.adj.true,
sei = sqrt(dpn$vi.adj.true),
par2is = "Tt",
tcrit = dpn$tcrit.adj.true,
Mu.start = Mhat.start,
par2.start = Shat.start,
usePrior = TRUE,
get.CIs = p$get.CIs,
CI.method = "wald",
run.optimx = p$run.optimx
),
.rep.res = rep.res )
Mhat.MAP = rep.res$Mhat[ rep.res$method == "jeffreys-adj-sd" ]
Shat.MAP = rep.res$Shat[ rep.res$method == "jeffreys-adj-sd" ]
}
# ~~ ******** MAON WITH CONFOUNDING ADJUSTMENT --------------------------------------
# using the sample estimate of muB
if ( "maon-adj-MhatB" %in% all.methods ) {
rep.res = run_method_safe(method.label = c("maon-adj-MhatB"),
method.fn = function() {
mod = robu( yi.adj.est ~ 1,
data = dpn,
studynum = 1:nrow(dpn),
var.eff.size = vi,  # using original variance
small = TRUE )
report_meta(mod, .mod.type = "robu")
},
.rep.res = rep.res )
}
# using the true muB
if ( "maon-adj-muB" %in% all.methods ) {
rep.res = run_method_safe(method.label = c("maon-adj-muB"),
method.fn = function() {
mod = robu( yi.adj.true ~ 1,
data = dpn,
studynum = 1:nrow(dpn),
var.eff.size = vi,  # using original variance
small = TRUE )
report_meta(mod, .mod.type = "robu")
},
.rep.res = rep.res )
}
# ~ Secondary/Sanity-Check Methods ------------------------------
# ~~ Naive (Unhacked Only)
if ( "prereg-naive" %in% all.methods &
nrow(dp.unhacked) > 0 ) {
rep.res = run_method_safe(method.label = c("prereg-naive"),
method.fn = function() {
mod = rma( yi = dp.unhacked$yi,
vi = dp.unhacked$vi,
method = "REML",
knha = TRUE )
report_meta(mod, .mod.type = "rma")
},
.rep.res = rep.res )
cat("\n doParallel flag: Done prereg-naive if applicable")
}
# ~ Add Scen Params and Sanity Checks --------------------------------------
# add in scenario parameters
# do NOT use rbind here; bind_cols accommodates possibility that some methods' rep.res
#  have more columns than others
rep.res = p %>% bind_cols( rep.res )
# add more info
rep.res = rep.res %>% add_column( rep.name = i, .before = 1 )
rep.res = rep.res %>% add_column( scen.name = scen, .before = 1 )
rep.res = rep.res %>% add_column( job.name = jobname, .before = 1 )
cat("\ndoParallel flag: Before adding sanity checks to rep.res")
# add info about simulated datasets
# "ustudies"/"udraws" refers to underlying studies/draws prior to hacking or publication bias
sancheck.prob.published.is.confounded = mean( dp$Ci == 1 )
sancheck.prob.published.affirm.is.confounded = mean( dp$Ci[ dp$affirm == 1 ] == 1 )
sancheck.prob.published.nonaffirm.is.confounded = mean( dp$Ci[ dp$affirm == 0 ] == 0 )
( sancheck.prob.ustudies.published =  mean( d.first$study %in% unique(dp$study) ) )
expect_equal( sancheck.prob.ustudies.published, nrow(dp)/nrow(d.first) )
# this one should always be 100% unless there's also publication bias:
( sancheck.prob.unhacked.ustudies.published =  mean( d.first$study[ d.first$hack == "no" ] %in% unique( dp$study[ dp$hack == "no" ] ) ) )
# under affirm hacking, will be <100%:
( sancheck.prob.hacked.ustudies.published =  mean( d.first$study[ d.first$hack != "no" ] %in% unique( dp$study[ dp$hack != "no" ] ) ) )
# might NOT be 100% if you're generating multiple draws per unhacked studies but favoring, e.g., a random one:
( sancheck.prob.unhacked.udraws.published =  mean( d$study.draw[ d$hack == "no" ] %in% unique( dp$study.draw[ dp$hack == "no" ] ) ) )
( sancheck.prob.hacked.udraws.published =  mean( d$study.draw[ d$hack != "no" ] %in% unique( dp$study.draw[ dp$hack != "no" ] ) ) )
#*this one is especially important: under worst-case hacking, it's analogous to prop.retained in
#  TNE since it's the proportion of the underlying distribution that's nonaffirmative
( sancheck.prob.unhacked.udraws.nonaffirm =  mean( d$affirm[ d$hack == "no" ] == FALSE ) )
# a benchmark for average power:
( sancheck.prob.unhacked.udraws.affirm =  mean( d$affirm[ d$hack == "no" ] ) )
( sancheck.prob.hacked.udraws.nonaffirm =  mean( d$affirm[ d$hack != "no" ] == FALSE ) )
( sancheck.prob.hacked.udraws.affirm =  mean( d$affirm[ d$hack != "no" ] ) )
# probability that a published, nonaffirmative draw is from a hacked study
# under worst-case hacking, should be 0
( sancheck.prob.published.nonaffirm.is.hacked = mean( dp$hack[ dp$affirm == 0 ] != "no" ) )
# this will be >0
( sancheck.prob.published.affirm.is.hacked = mean( dp$hack[ dp$affirm == 1 ] != "no" ) )
# average yi's
rep.res = rep.res %>% add_column(   sancheck.dp.k = nrow(dp),
sancheck.dp.k.affirm = sum(dp$affirm == TRUE),
sancheck.dp.k.nonaffirm = sum(dp$affirm == FALSE),
sancheck.prob.published.is.confounded = sancheck.prob.published.is.confounded,
sancheck.prob.published.affirm.is.confounded = sancheck.prob.published.affirm.is.confounded,
sancheck.prob.published.nonaffirm.is.confounded = sancheck.prob.published.nonaffirm.is.confounded,
sancheck.dp.k.affirm.unhacked = sum(dp$affirm == TRUE & dp$hack == "no"),
sancheck.dp.k.affirm.hacked = sum(dp$affirm == TRUE & dp$hack != "no"),
sancheck.dp.k.nonaffirm.unhacked = sum(dp$affirm == FALSE & dp$hack == "no"),
sancheck.dp.k.nonaffirm.hacked = sum(dp$affirm == FALSE & dp$hack != "no"),
# means draws per HACKED, published study
sancheck.dp.meanN.hacked = mean( dp$N[dp$hack != "no"] ),
sancheck.dp.q90N.hacked = quantile( dp$N[dp$hack != "no"], 0.90 ),
# average yi's of published draws from each study type
sancheck.mean.yi.unhacked.pub.study = mean( dp$yi[ dp$hack == "no"] ),
sancheck.mean.yi.hacked.pub.study = mean( dp$yi[ dp$hack != "no"] ),
sancheck.mean.mui.unhacked.pub.nonaffirm = mean( dp$mui[ dp$hack == "no" & dp$affirm == FALSE ] ),
sancheck.mean.yi.unhacked.pub.nonaffirm = mean( dp$yi[ dp$hack == "no" & dp$affirm == FALSE ] ),
sancheck.mean.yi.unhacked.pub.affirm = mean( dp$yi[ dp$hack == "no" & dp$affirm == TRUE ] ),
sancheck.mean.yi.hacked.pub.nonaffirm = mean( dp$yi[ dp$hack != "no" & dp$affirm == FALSE ] ),
sancheck.mean.yi.hacked.pub.affirm = mean( dp$yi[ dp$hack != "no" & dp$affirm == TRUE ] ),
# average Zi's
sancheck.mean.Zi.unhacked.pub.study = mean( dp$Zi[ dp$hack == "no"] ),
sancheck.mean.Zi.hacked.pub.study = mean( dp$Zi[ dp$hack != "no"] ),
sancheck.mean.Zi.unhacked.pub.nonaffirm = mean( dp$Zi[ dp$hack == "no" & dp$affirm == FALSE ] ),
sancheck.mean.Zi.unhacked.pub.affirm = mean( dp$Zi[ dp$hack == "no" & dp$affirm == TRUE ] ),
sancheck.mean.Zi.hacked.pub.nonaffirm = mean( dp$Zi[ dp$hack != "no" & dp$affirm == FALSE ] ),
sancheck.mean.Zi.hacked.pub.affirm = mean( dp$Zi[ dp$hack != "no" & dp$affirm == TRUE ] ),
sancheck.prob.ustudies.published = sancheck.prob.ustudies.published,
sancheck.prob.unhacked.ustudies.published = sancheck.prob.unhacked.ustudies.published,
sancheck.prob.hacked.ustudies.published = sancheck.prob.hacked.ustudies.published,
sancheck.prob.unhacked.udraws.published = sancheck.prob.unhacked.udraws.published,
sancheck.prob.hacked.udraws.published = sancheck.prob.hacked.udraws.published,
sancheck.prob.unhacked.udraws.nonaffirm = sancheck.prob.unhacked.udraws.nonaffirm,
sancheck.prob.unhacked.udraws.affirm = sancheck.prob.unhacked.udraws.affirm,
sancheck.prob.hacked.udraws.nonaffirm = sancheck.prob.hacked.udraws.nonaffirm,
sancheck.prob.hacked.udraws.affirm = sancheck.prob.hacked.udraws.affirm,
sancheck.prob.published.nonaffirm.is.hacked = sancheck.prob.published.nonaffirm.is.hacked,
# sanity checks for gamma, the bias adjustment
# E[Bi^* | Ci^* = 1], the target for gamma:
sancheck.MhatB = MhatB,
#@note: Di = 1 here is FAVORING indicator, so this is still the mean Bi among underlying (pre-SAS) estimates
# this is an underlying SAMPLE estimate of the truth; should approximately agree with MhatB and muB
sancheck.EBsti = mean( d$Bi[ d$Ci == 1 & d$Di == 1] )
)
rep.res
}  ### end foreach loop
} )[3]  # end system.time
dim(rs)
# quick look
rs %>% mutate(MhatWidth = MHi - MLo,
MhatCover = as.numeric( MHi > Mu & MLo < Mu ) ) %>%
dplyr::select(method, Mhat, MhatWidth, MhatCover,
sancheck.MhatB, sancheck.EBsti) %>%
group_by(method) %>%
summarise_if(is.numeric, function(x) round( meanNA(x), 2 ) )
# # LOCAL
# # how badly biased are the hacked studies?
# temp = rs %>% filter(method == "naive")
# mean(temp$sancheck.mean.yi.hacked.pub.study)
# mean(temp$sancheck.mean.yi.unhacked.pub.study)
#
# mean(temp$sancheck.mean.yi.unhacked.pub.affirm)
# mean(temp$sancheck.mean.yi.hacked.pub.affirm)
#
# mean(temp$sancheck.mean.yi.unhacked.pub.nonaffirm)
# mean(temp$sancheck.mean.yi.hacked.pub.nonaffirm)
# ~~ End of ForEach Loop ----------------
# data-wrangling packages
library(here)
library(plotly)  # must be BEFORE dplyr or else plotly::select will take over
library(dplyr)
library(tibble)
library(ggplot2)
library(data.table)
library(tidyverse)
library(fastDummies)
library(xlsx)
# meta-analysis packages
library(metafor)
library(robumeta)
# other
library(xtable)
library(testthat)
library(Deriv)
library(mosaic)
library(hpa)
library(pracma)
library(truncnorm)
library(tmvtnorm)
library(RColorBrewer)
library(sjmisc)
# prevent masking
select = dplyr::select
# ~~ User-specified global vars -------------------------
# no sci notation
options(scipen=999)
# control which results should be redone and/or overwritten
#@ not all fns respect this setting
overwrite.res = FALSE
# ~~ Set directories -------------------------
code.dir = here()
data.dir = str_replace( string = here(),
pattern = "Code",
replacement = "Results" )
results.dir = str_replace( string = here(),
pattern = "Code",
replacement = "Results" )
overleaf.dir.figs = "/Users/mmathur/Dropbox/Apps/Overleaf/Multiple-bias meta-analysis Overleaf (MBMA)/figures/sims"
# # alternative for running new simulations
# data.dir = str_replace( string = here(),
#                         pattern = "Code \\(git\\)",
#                         replacement = "Simulation results" )
#
# results.dir = data.dir
setwd(code.dir)
source("helper_MBMA.R")
source("analyze_sims_helper_MBMA.R")
# if only analyzing a single set of sims (no merging):
setwd(data.dir)
agg = fread( "agg.csv")
# check when the dataset was last modified to make sure we're working with correct version
file.info("agg.csv")$mtime
dim(agg)  # will exceed number of scens because of multiple methods
#expect_equal( 84, nuni(agg$scen.name) )
# prettify variable names
agg = wrangle_agg_local(agg)
# look at number of actual sim reps
table(agg$sim.reps.actual)
# ~~ List variable names -------------------------
# initialize global variables that describe estimate and outcome names, etc.
# this must be after calling wrangle_agg_local
init_var_names()
# BEST AND WORST PERFORMANCE IN CORRECTLY-SPECIFIED SCENS -------------------------
# scens where RTMA is correctly specified, but all other methods aren't
as.data.frame( agg %>% filter( hack != "affirm2" ) %>%
filter( !( method %in% c("rtma-adj-pmean", "rtma-adj-pmed") ) ) %>%
group_by(method) %>%
summarise( min(MhatBias),
median(MhatBias),
max(MhatBias),
min(MhatCover),
median(MhatCover)) )
table(agg$prob.hacked)
table(agg$hack)
table(is.na(agg$Mhat), agg$method)
agg %>% filter(method == "sapb-adj-MhatB") %>%
group_by(prob.conf) %>%
summarise( MhatEstFail )
agg %>% filter(method == "sapb-adj-MhatB") %>%
group_by(prob.conf) %>%
summarise( mean(MhatEstFail) )
# TEMP: REMOVE SCENS WITH NO CONFOUNDED STUDIES OR WITH HACKING
agg %>% filter(prob.conf > 0 & prob.hacked == 0)
# TEMP: REMOVE SCENS WITH NO CONFOUNDED STUDIES OR WITH HACKING
agg = agg %>% filter(prob.conf > 0 & prob.hacked == 0)
dim(agg)
agg$prob.conf
Ynames = rev(MhatYNames)
# alternatively, run just a subset:
Ynames = c("MhatWidth", "MhatCover", "MhatBias",
"MhatEstFail")
# to help decide which vars to include in plot:
param.vars.manip2
# to help decide which vars to include in plot:
param.vars.manip2
# in case you want to filter scens:
# full set for reference:
# c("naive", "gold-std", "maon", "2psm", "pcurve", "jeffreys-mcmc-pmean",
#   "jeffreys-mcmc-pmed", "jeffreys-mcmc-max-lp-iterate", "jeffreys-sd",
#   "jeffreys-var", "mle-sd", "csm-mle-sd", "mle-var", "2psm-csm-dataset",
#   "prereg-naive", "ltn-mle-sd")
( all.methods = unique(agg$method) )
#toDrop = c("jeffreys-mcmc-pmean", "jeffreys-mcmc-max-lp-iterate")
toDrop = NULL
method.keepers = all.methods[ !all.methods %in% toDrop ]
# for each hacking method and Mu, make facetted plotly
for ( .hack in unique(agg$hack) ) {
for ( .t2a in unique(agg$t2a) ) {
# # test only
# .hack = "affirm"
# .t2a = 0.04
cat( paste("\n\n -------- STARTING t2a=", .t2a, ", hack=", .hack, sep = "") )
aggp = agg %>% filter(method %in% method.keepers &
t2a == .t2a &
hack == .hack)
# to label the plots
prefix = paste( "2022-6-19 sims; ",
"t2a=", .t2a,
"; hack=", .hack,
sep = "")
# temporarily set wd
# results.dir.temp = paste(results.dir,
#                          "/Big unprettified plots/",
#                          .Mu,
#                          "/hack=",
#                          .hack,
#                          sep = "")
results.dir.temp = paste(results.dir,
"/Big unprettified plots",
sep = "")
# set facetting variables for plots
aggp$tempFacetVar1 = paste( "prob.hack=", aggp$prob.hacked,
sep = "")
aggp$tempFacetVar2 = paste( "prob.conf=", aggp$prob.conf,
"; eta=", aggp$eta,
#"; t2a=", aggp$t2a,
sep = "")
table(aggp$tempFacetVar2)
for ( Yname in Ynames) {
# to run "manually"
#Yname = "MhatBias"
#Yname = "MhatCover"
y.breaks = NULL
if ( Yname == "MhatBias") y.breaks = seq(-0.5, 1, 0.1)
if ( Yname == "MhatWidth") y.breaks = seq(0, 10, 0.5)
p  = quick_5var_agg_plot(.Xname = "k.pub.nonaffirm",
.Yname = Yname,
.colorVarName = "method",
.facetVar1Name = "tempFacetVar1",
.facetVar2Name = "tempFacetVar2",
.dat = aggp,
.ggtitle = prefix,
.y.breaks = y.breaks,
.writePlot = FALSE,
.results.dir = results.dir.temp)
pl = ggplotly(p)
# in filename, mark the most important plots with asterisk
if ( Yname %in% c("MhatBias", "MhatCover", "MhatWidth") ){
new.prefix = paste("*", prefix, sep = "")
} else {
new.prefix = prefix
}
# how to save a plotly as html
# https://www.biostars.org/p/458325/
setwd(results.dir.temp)
string = paste(new.prefix, Yname, "plotly.html", sep="_")
htmlwidgets::saveWidget(pl, string)
}
}
}
# TEMP: REMOVE SCENS WITH NO CONFOUNDED STUDIES OR WITH HACKING
agg = agg %>% filter(prob.conf > 0 & prob.hacked == 0 &
hack.type == "affirm")
# TEMP: REMOVE SCENS WITH NO CONFOUNDED STUDIES OR WITH HACKING
agg = agg %>% filter(prob.conf > 0 & prob.hacked == 0 &
hack == "affirm")
dim(agg)
Ynames = rev(MhatYNames)
# alternatively, run just a subset:
Ynames = c("MhatWidth", "MhatCover", "MhatBias",
"MhatEstFail")
# to help decide which vars to include in plot:
param.vars.manip2
# in case you want to filter scens:
# full set for reference:
# c("naive", "gold-std", "maon", "2psm", "pcurve", "jeffreys-mcmc-pmean",
#   "jeffreys-mcmc-pmed", "jeffreys-mcmc-max-lp-iterate", "jeffreys-sd",
#   "jeffreys-var", "mle-sd", "csm-mle-sd", "mle-var", "2psm-csm-dataset",
#   "prereg-naive", "ltn-mle-sd")
( all.methods = unique(agg$method) )
#toDrop = c("jeffreys-mcmc-pmean", "jeffreys-mcmc-max-lp-iterate")
toDrop = NULL
method.keepers = all.methods[ !all.methods %in% toDrop ]
# for each hacking method and Mu, make facetted plotly
for ( .hack in unique(agg$hack) ) {
for ( .t2a in unique(agg$t2a) ) {
# # test only
# .hack = "affirm"
# .t2a = 0.04
cat( paste("\n\n -------- STARTING t2a=", .t2a, ", hack=", .hack, sep = "") )
aggp = agg %>% filter(method %in% method.keepers &
t2a == .t2a &
hack == .hack)
# to label the plots
prefix = paste( "2022-6-19 sims; ",
"t2a=", .t2a,
"; hack=", .hack,
sep = "")
# temporarily set wd
# results.dir.temp = paste(results.dir,
#                          "/Big unprettified plots/",
#                          .Mu,
#                          "/hack=",
#                          .hack,
#                          sep = "")
results.dir.temp = paste(results.dir,
"/Big unprettified plots",
sep = "")
# set facetting variables for plots
aggp$tempFacetVar1 = paste( "prob.hack=", aggp$prob.hacked,
sep = "")
aggp$tempFacetVar2 = paste( "prob.conf=", aggp$prob.conf,
"; eta=", aggp$eta,
#"; t2a=", aggp$t2a,
sep = "")
table(aggp$tempFacetVar2)
for ( Yname in Ynames) {
# to run "manually"
#Yname = "MhatBias"
#Yname = "MhatCover"
y.breaks = NULL
if ( Yname == "MhatBias") y.breaks = seq(-0.5, 1, 0.1)
if ( Yname == "MhatWidth") y.breaks = seq(0, 10, 0.5)
p  = quick_5var_agg_plot(.Xname = "k.pub.nonaffirm",
.Yname = Yname,
.colorVarName = "method",
.facetVar1Name = "tempFacetVar1",
.facetVar2Name = "tempFacetVar2",
.dat = aggp,
.ggtitle = prefix,
.y.breaks = y.breaks,
.writePlot = FALSE,
.results.dir = results.dir.temp)
pl = ggplotly(p)
# in filename, mark the most important plots with asterisk
if ( Yname %in% c("MhatBias", "MhatCover", "MhatWidth") ){
new.prefix = paste("*", prefix, sep = "")
} else {
new.prefix = prefix
}
# how to save a plotly as html
# https://www.biostars.org/p/458325/
setwd(results.dir.temp)
string = paste(new.prefix, Yname, "plotly.html", sep="_")
htmlwidgets::saveWidget(pl, string)
}
}
}
